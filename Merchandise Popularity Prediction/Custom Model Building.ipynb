{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\sunil\\\\Projects\\\\Machine Hack\\\\Merchandise Popularity Prediction\\\\Dataset'\n",
    "\n",
    "train = pd.read_csv(path + '\\\\Train.csv')\n",
    "test = pd.read_csv(path + '\\\\Test.csv')\n",
    "sample_sub = pd.read_csv(path + '\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'popularity'\n",
    "features = [col for col in train.columns if col not in [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, val = train_test_split(train, test_size=0.2, random_state = 1, stratify = train[target])\n",
    "\n",
    "###### Input to our model will be the features\n",
    "X_trn, X_val = trn[features], val[features]\n",
    "\n",
    "###### Output of our model will be the TARGET_COL\n",
    "y_trn, y_val = trn[target], val[target]\n",
    "\n",
    "##### Features for the test data that we will be predicting\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3436662428845033"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(random_state = 1,max_depth = 35, n_estimators = 2000)\n",
    "_ = clf.fit(X_trn, y_trn)\n",
    "\n",
    "preds_val = clf.predict_proba(X_val)\n",
    "\n",
    "log_loss(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3983739765664552"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state = 1)\n",
    "_ = clf.fit(X_trn, y_trn)\n",
    "\n",
    "preds_val = clf.predict_proba(X_val)\n",
    "\n",
    "log_loss(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear', 'poly', 'precomputed', 'rbf', 'sigmoid'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(preds)\n",
    "sample.to_csv(path+'\\\\xgb_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(regressor, train, test, features):\n",
    "    N_splits = 5\n",
    "    \n",
    "    oofs = np.zeros(shape=(len(train), 5))\n",
    "    preds = np.zeros(shape=(len(test), 5))\n",
    "    \n",
    "    target_col = train[target]\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits = N_splits, shuffle = True,random_state = 1999)\n",
    "    stratified_target = pd.qcut( train[target], 10, labels=False, duplicates='drop')\n",
    "    \"\"\n",
    "    for index, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n",
    "        print(f'\\n================================Fold{index + 1}===================================')\n",
    "        \n",
    "        #### Train Set\n",
    "        X_trn, y_trn = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n",
    "        \n",
    "        #### Validation Set\n",
    "        X_val, y_val = train[features].iloc[val_idx], train[target].iloc[val_idx]\n",
    "        \n",
    "        #### Test Set\n",
    "        X_test = test[features]\n",
    "        \n",
    "        #### Scaling Data ####\n",
    "        #scaler = StandardScaler()\n",
    "        #_ = scaler.fit(X_trn)\n",
    "        #\n",
    "        #X_trn = scaler.transform(X_trn)\n",
    "        #X_val = scaler.transform(X_val)\n",
    "        #X_test = scaler.transform(X_test)\n",
    "        \n",
    "        ############ Fitting #############\n",
    "        _ = regressor.fit(X_trn, y_trn)\n",
    "        \n",
    "        ############ Predicting #############\n",
    "        val_preds = regressor.predict_proba(X_val)\n",
    "        test_preds = regressor.predict_proba(X_test)\n",
    "        \n",
    "        error = log_loss(y_val, val_preds)\n",
    "        print(f'\\n Logloss for Validation set is : {error}')\n",
    "        \n",
    "        oofs[val_idx] = val_preds\n",
    "        preds += test_preds / N_splits\n",
    "        \n",
    "    total_error = log_loss(target_col, oofs)\n",
    "    print(f'\\n\\Logloss for oofs is {total_error}')\n",
    "    \n",
    "    return oofs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================Fold1===================================\n",
      "\n",
      " Logloss for Validation set is : 0.43849472942407924\n",
      "\n",
      "================================Fold2===================================\n",
      "\n",
      " Logloss for Validation set is : 0.44175756392152277\n",
      "\n",
      "================================Fold3===================================\n",
      "\n",
      " Logloss for Validation set is : 0.44294865367705233\n",
      "\n",
      "================================Fold4===================================\n",
      "\n",
      " Logloss for Validation set is : 0.460012143727871\n",
      "\n",
      "================================Fold5===================================\n",
      "\n",
      " Logloss for Validation set is : 0.4419423854521928\n",
      "\n",
      "\\Logloss for oofs is 0.44503064649924085\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_oofs, lr_preds = cross_val(clf, trn, test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'popularity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = train.copy()\n",
    "t1 = train.copy()\n",
    "t3 = train.copy()\n",
    "t4 = train.copy()\n",
    "t5 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0[target] = t0[target].apply(lambda x: 1 if x==0 else 0)\n",
    "t1[target] = t1[target].apply(lambda x: 1 if x==1 else 0)\n",
    "t3[target] = t3[target].apply(lambda x: 1 if x==3 else 0)\n",
    "t4[target] = t4[target].apply(lambda x: 1 if x==4 else 0)\n",
    "t5[target] = t5[target].apply(lambda x: 1 if x==5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18192\n",
       "1       16\n",
       "Name: popularity, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ls = [t0, t1, t3, t4, t5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_ls = ['0', '1', '2','3', '4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_proba_df = pd.DataFrame()\n",
    "test_proba_df = pd.DataFrame()\n",
    "\n",
    "train_probas = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for t0 is 0.00010766464530425834\n",
      "loss on full train is 0.00011535138527560551\n",
      "\n",
      "log loss for t1 is 0.06863565437490646\n",
      "loss on full train is 0.020560055336419258\n",
      "\n",
      "log loss for t2 is 0.29073961084829725\n",
      "loss on full train is 0.15189471148673692\n",
      "\n",
      "log loss for t3 is 0.3199145406412955\n",
      "loss on full train is 0.18098456372179494\n",
      "\n",
      "log loss for t4 is 0.098201012075115\n",
      "loss on full train is 0.0318966199152879\n",
      "\n",
      "Total loss is 0.7775984825849184\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for name, df in zip(df_name_ls,df_ls):\n",
    "    trn, val = train_test_split(df, test_size=0.2, random_state = 1, stratify = df[target])\n",
    "\n",
    "    ###### Input to our model will be the features\n",
    "    X_trn, X_val = trn[features], val[features]\n",
    "\n",
    "    ###### Output of our model will be the TARGET_COL\n",
    "    y_trn, y_val = trn[target], val[target]\n",
    "\n",
    "    ##### Features for the test data that we will be predicting\n",
    "    X_test = test[features]\n",
    "    \n",
    "    ## Oversampling Using SMOTE\n",
    "    #sm = SMOTE(random_state=1)\n",
    "    #X_trn_os, y_trn_os = sm.fit_sample(X_trn, y_trn.ravel())\n",
    "    \n",
    "    ## Training and Predicting\n",
    "    clf = XGBClassifier(random_state=1)\n",
    "    _ = clf.fit(X_trn, y_trn)\n",
    "    val_preds = clf.predict_proba(X_val)[:,1]\n",
    "    loss = log_loss(y_val, val_preds)\n",
    "    total_loss+=loss\n",
    "    print(f'log loss for t{name} is {loss}')\n",
    "    \n",
    "    # Predicting on full train\n",
    "    trn_preds = clf.predict_proba(df[features])[:, 1]\n",
    "    print(f'loss on full train is {log_loss(df[target], trn_preds)}')\n",
    "    target_proba_df[name] = val_preds\n",
    "    train_probas[int(name)] = trn_preds\n",
    "    \n",
    "    test_preds = clf.predict_proba(X_test)[:,1]\n",
    "    test_proba_df[int(name)] = test_preds\n",
    "    print()\n",
    "print(f'Total loss is {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probas[target] = train_probas.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [0, 1, 2, 3, 4]\n",
    "trn, val = train_test_split(train_probas, test_size=0.2, random_state = 1, stratify = train_probas[target])\n",
    "\n",
    "###### Input to our model will be the features\n",
    "X_trn, X_val = trn[features], val[features]\n",
    "\n",
    "###### Output of our model will be the TARGET_COL\n",
    "y_trn, y_val = trn[target], val[target]\n",
    "\n",
    "##### Features for the test data that we will be predicting\n",
    "X_test = test_proba_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=1)\n",
    "clf.fit(X_trn, y_trn)\n",
    "\n",
    "preds = clf.predict_proba(X_val)\n",
    "loss = log_loss(y_val, preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(trn[features], trn[target])\n",
    "preds = clf.predict_proba(X_test)\n",
    "\n",
    "sample = pd.DataFrame(preds)\n",
    "sample.to_csv(path+'\\\\normal self build model 1st try.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tasks for getting proba sum = 1:\n",
    "    1. make cross validation inside for loop.\n",
    "    2. Take probabilities for whole train set.\n",
    "    3. train model on train probabilities and take predictions on test probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, df in zip(df_name_ls,df_ls):\n",
    "    \n",
    "    oofs = np.zeros(len(df))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state=1)\n",
    "    stratified_target = pd.qcut( df[target], 10, label=False, duplicates='drop')\n",
    "    \n",
    "    for index, (trn_idx, val_idx) in enumerate(flods.split(df, stratified_target)):\n",
    "        X_trn, y_trn = df[features].iloc[trn_idx], df[target].iloc[trn_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = test_proba_df.idxmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
