{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "seed = 1\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\sunil\\\\Projects\\\\Machine Hack\\\\Merchandise Popularity Prediction\\\\Dataset'\n",
    "\n",
    "train = pd.read_csv(path + '\\\\Train.csv')\n",
    "test = pd.read_csv(path + '\\\\Test.csv')\n",
    "sample_sub = pd.read_csv(path + '\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'popularity'\n",
    "features = [col for col in train.columns if col not in [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[target].replace({0:0, 1:1, 3:2, 4:3, 5:4}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Predicting For Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fill_Duplicates(row):\n",
    "    tmp = train[ (train['Store_Ratio'] == row['Store_Ratio']) &\n",
    "                 (train['Basket_Ratio'] == row['Basket_Ratio']) &\n",
    "                 (train['Category_1'] == row['Category_1']) &\n",
    "                 (train['Store_Score'] == row['Store_Score']) &\n",
    "                 (train['Category_2'] == row['Category_2']) &\n",
    "                 (train['Store_Presence'] == row['Store_Presence']) &\n",
    "                 (train['Score_1'] == row['Score_1']) & \n",
    "                 (train['Score_2'] == row['Score_2']) &\n",
    "                 (train['Score_3'] == row['Score_3']) &\n",
    "                 (train['Score_4'] == row['Score_4'])]\n",
    "    if tmp.shape[0] == 0:\n",
    "        return None\n",
    "    return tmp[target].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target] = test.apply(lambda x: Fill_Duplicates(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store_Ratio          0\n",
       "Basket_Ratio         0\n",
       "Category_1           0\n",
       "Store_Score          0\n",
       "Category_2           0\n",
       "Store_Presence       0\n",
       "Score_1              0\n",
       "Score_2              0\n",
       "Score_3              0\n",
       "Score_4              0\n",
       "time                 0\n",
       "popularity        8802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_test_index = test[~test[target].isna()].index\n",
    "filled_test_values = test[~test[target].isna()][target].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3338"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filled_test_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Store Ratio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_1 = train[ train[target] == 1]['Store_Ratio'].unique()\n",
    "store_2 = train[ train[target] == 2]['Store_Ratio'].unique()\n",
    "store_3 = train[ train[target] == 3]['Store_Ratio'].unique()\n",
    "store_4 = train[ train[target] == 4]['Store_Ratio'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Popularity 1\n",
    "diff_1 = np.setdiff1d(store_1, store_2)\n",
    "diff_2 = np.setdiff1d(store_1, store_3)\n",
    "diff_3 = np.setdiff1d(store_1, store_4)\n",
    "\n",
    "a = list(np.concatenate((diff_1, diff_2, diff_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_a = list(set(a))\n",
    "store_ratio_for_1 = []\n",
    "for i in set_a:\n",
    "    if a.count(i) == 3:\n",
    "        store_ratio_for_1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Popularity 2\n",
    "diff_1 = np.setdiff1d(store_2, store_1)\n",
    "diff_2 = np.setdiff1d(store_2, store_3)\n",
    "diff_3 = np.setdiff1d(store_2, store_4)\n",
    "\n",
    "b = list(np.concatenate((diff_1, diff_2, diff_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_b = list(set(b))\n",
    "store_ratio_for_2 = []\n",
    "for i in set_b:\n",
    "    if b.count(i) == 3:\n",
    "        store_ratio_for_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Popularity 3\n",
    "diff_1 = np.setdiff1d(store_3, store_1)\n",
    "diff_2 = np.setdiff1d(store_3, store_2)\n",
    "diff_3 = np.setdiff1d(store_3, store_4)\n",
    "\n",
    "c = list(np.concatenate((diff_1, diff_2, diff_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_c = list(set(c))\n",
    "store_ratio_for_3 = []\n",
    "for i in set_c:\n",
    "    if c.count(i) == 3:\n",
    "        store_ratio_for_3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Popularity 4\n",
    "diff_1 = np.setdiff1d(store_4, store_1)\n",
    "diff_2 = np.setdiff1d(store_4, store_2)\n",
    "diff_3 = np.setdiff1d(store_4, store_3)\n",
    "\n",
    "d = list(np.concatenate((diff_1, diff_2, diff_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_d = list(set(d))\n",
    "store_ratio_for_4 = []\n",
    "for i in set_d:\n",
    "    if d.count(i) == 3:\n",
    "        store_ratio_for_4.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_by_store_ratio(x):\n",
    "    if x in store_ratio_for_1:\n",
    "        return 1\n",
    "    elif x in store_ratio_for_2:\n",
    "        return 2\n",
    "    elif x in store_ratio_for_4:\n",
    "        return 4\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.loc[test[target].isna(), target] = test[ test[target].isna()]['Store_Ratio'].apply(lambda x : fill_by_store_ratio(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, val = train_test_split(train, test_size = 0.2, random_state = 1, stratify = train[target])\n",
    "\n",
    "##### Input for model\n",
    "X_trn, X_val = trn[features], val[features]\n",
    "\n",
    "##### Target column\n",
    "y_trn, y_val = trn[target], val[target]\n",
    "\n",
    "##### Features for test data that we will be predicting\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3983739765664552"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state = 1)\n",
    "_ = clf.fit(X_trn, y_trn)\n",
    "\n",
    "preds_val = clf.predict_proba(X_val)\n",
    "\n",
    "log_loss(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3436662428845033"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(random_state = 1,max_depth = 35, n_estimators = 2000)\n",
    "_ = clf.fit(X_trn, y_trn)\n",
    "\n",
    "preds_val = clf.predict_proba(X_val)\n",
    "\n",
    "log_loss(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Probabilities for Manually Predicted Indexes\n",
    "def Manual_Probability_Filling():\n",
    "    for i, (index,target_value) in enumerate(zip(preds[filled_test_index], filled_test_values)):\n",
    "        tmp_ls = []\n",
    "        tmp_ls = [v for v in range(0,5) if v not in [target_value]]\n",
    "        preds[filled_test_index[i]][int(target_value)] = 1\n",
    "        for v in tmp_ls:\n",
    "            preds[filled_test_index[i]][int(v)] = 0\n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.zeros((len(preds), 1))\n",
    "#preds = np.append(a,preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[target] = test.apply(lambda x: Fill_Duplicates(x), axis = 1)\n",
    "\n",
    "filled_test_index = test[~test[target].isna()].index\n",
    "filled_test_values = test[~test[target].isna()][target].to_list()\n",
    "\n",
    "Manual_Probability_Filling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8802"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[target].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(preds)\n",
    "sample.to_csv(path+'\\\\Extra_Tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf, train, test, features):\n",
    "    N_splits = 5\n",
    "    \n",
    "    oofs = np.zeros((len(train), 5))\n",
    "    preds = np.zeros((len(test), 5))\n",
    "    preds_acc = np.zeros(len(train))\n",
    "    \n",
    "    target_col = train[target]\n",
    "    folds = StratifiedKFold(N_splits, shuffle = True,random_state = seed)\n",
    "    stratidied_target = pd.qcut(target_col, 10, labels=False, duplicates='drop')\n",
    "    \n",
    "    for index, (trn_idx, val_idx) in enumerate(folds.split(train, stratidied_target)):\n",
    "        print(f'*********************Fold {index+1}*********************')\n",
    "        \n",
    "        ## Training Set\n",
    "        X_trn, y_trn = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n",
    "        \n",
    "        ## validation Set\n",
    "        X_val, y_val = train[features].iloc[val_idx], train[target].iloc[val_idx]\n",
    "        \n",
    "        ## Test Set\n",
    "        X_test = test[features]\n",
    "        \n",
    "        ## Scaling Data\n",
    "        scaler = StandardScaler()\n",
    "        _ = scaler.fit(X_trn)\n",
    "        X_trn = scaler.transform(X_trn)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        ## Fitting\n",
    "        _ = clf.fit(X_trn, y_trn)\n",
    "        \n",
    "        ############# Predicting\n",
    "        \n",
    "        ##Probabilities\n",
    "        val_preds_probs = clf.predict_proba(X_val)\n",
    "        test_preds_probs = clf.predict_proba(X_test)\n",
    "        \n",
    "        ## Classes\n",
    "        val_preds = clf.predict(X_val)\n",
    "        \n",
    "        loss = log_loss(y_val, val_preds_probs)\n",
    "        print(f'Log Loss is {loss}')\n",
    "        print()\n",
    "        oofs[val_idx] = val_preds_probs\n",
    "        preds += test_preds_probs/N_splits\n",
    "        preds_acc[val_idx] = val_preds\n",
    "        \n",
    "    total_loss = log_loss(target_col, oofs)\n",
    "    print(f'Log Loss on All Train Set is {total_loss}')\n",
    "    \n",
    "    return oofs, preds, preds_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boosting(clf, train, test, features, params):\n",
    "    N_splits = 5\n",
    "    \n",
    "    oofs = np.zeros((len(train), 5))\n",
    "    preds = np.zeros((len(test), 5))\n",
    "    preds_acc = np.zeros(len(train))\n",
    "    \n",
    "    target_col = train[target]\n",
    "    folds = StratifiedKFold(N_splits, shuffle = True,random_state = seed)\n",
    "    stratidied_target = pd.qcut(target_col, 10, labels=False, duplicates='drop')\n",
    "    \n",
    "    for index, (trn_idx, val_idx) in enumerate(folds.split(train, stratidied_target)):\n",
    "        print(f'*********************Fold {index+1}*********************')\n",
    "        \n",
    "        ## Training Set\n",
    "        X_trn, y_trn = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n",
    "        \n",
    "        ## validation Set\n",
    "        X_val, y_val = train[features].iloc[val_idx], train[target].iloc[val_idx]\n",
    "        \n",
    "        ## Test Set\n",
    "        X_test = test[features]\n",
    "        \n",
    "        ## Scaling Data\n",
    "        scaler = StandardScaler()\n",
    "        _ = scaler.fit(X_trn)\n",
    "        X_trn = scaler.transform(X_trn)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        ## Fitting\n",
    "        _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **params)\n",
    "        \n",
    "        ############# Predicting\n",
    "        \n",
    "        ##Probabilities\n",
    "        val_preds_probs = clf.predict_proba(X_val)\n",
    "        test_preds_probs = clf.predict_proba(X_test)\n",
    "        \n",
    "        ## Classes\n",
    "        val_preds = clf.predict(X_val)\n",
    "        \n",
    "        loss = log_loss(y_val, val_preds_probs)\n",
    "        print(f'Log Loss is {loss}')\n",
    "        print()\n",
    "        oofs[val_idx] = val_preds_probs\n",
    "        preds += test_preds_probs/N_splits\n",
    "        preds_acc[val_idx] = val_preds\n",
    "        \n",
    "    total_loss = log_loss(target_col, oofs)\n",
    "    print(f'Log Loss on All Train Set is {total_loss}')\n",
    "    \n",
    "    return oofs, preds, preds_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(random_state = 1999,max_depth = 35, n_estimators = 2000)\n",
    "#clf = LGBMClassifier(random_state = 1, n_estimators=1000, learning_rate = 0.02, n_jobs=-1, colsample_bytree=0.6)\n",
    "#clf = XGBClassifier(random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Fold 1*********************\n",
      "Log Loss is 0.35467023914898205\n",
      "\n",
      "*********************Fold 2*********************\n",
      "Log Loss is 0.3355618504665808\n",
      "\n",
      "*********************Fold 3*********************\n",
      "Log Loss is 0.3602692932579629\n",
      "\n",
      "*********************Fold 4*********************\n",
      "Log Loss is 0.3564148237954403\n",
      "\n",
      "*********************Fold 5*********************\n",
      "Log Loss is 0.34957438694909715\n",
      "\n",
      "Log Loss on All Train Set is 0.3512979323784184\n"
     ]
    }
   ],
   "source": [
    "params = {'verbose': 100, 'early_stopping_rounds': 50}\n",
    "oofs, preds, preds_acc = cross_val(clf, train, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(train[target], preds_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f109b36b88>"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm = pd.DataFrame(confusion, index = [i for i in \"01234\"],\n",
    "                  columns = [i for i in \"01234\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[target] = test.apply(lambda x: Fill_Duplicates(x), axis = 1)\n",
    "\n",
    "filled_test_index = test[~test[target].isna()].index\n",
    "filled_test_values = test[~test[target].isna()][target].to_list()\n",
    "\n",
    "Manual_Probability_Filling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, a in enumerate(preds):\n",
    "    for i in range(len(a)):\n",
    "        preds[index][i] = float(preds[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 7.90000000e-03, 2.78034483e-02, 9.55596552e-01,\n",
       "        8.70000000e-03],\n",
       "       [0.00000000e+00, 4.50000000e-03, 5.57000000e-02, 9.18600000e-01,\n",
       "        2.12000000e-02],\n",
       "       ...,\n",
       "       [0.00000000e+00, 3.90000000e-03, 1.58500000e-02, 9.79050000e-01,\n",
       "        1.20000000e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [2.00000000e-04, 3.81000000e-02, 2.49600000e-01, 6.93500000e-01,\n",
       "        1.86000000e-02]])"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(preds)\n",
    "sample.to_csv(path+'\\\\cross_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df(train, test):\n",
    "    df = pd.concat([train, test], axis = 0).reset_index(drop = True)\n",
    "    features = [col for col in df.columns if col not in [target]]\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "def split_df(df):\n",
    "    train_proc, test_proc = df[:train.shape[0]].reset_index(drop=True), df[train.shape[0]:].reset_index(drop = True)\n",
    "    features = [col for col in df.columns if col not in [target]]\n",
    "    \n",
    "    return train_proc, test_proc, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, features = join_df(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Score_2'] = np.log(df['Score_2']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = np.log(df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc, test_proc, features = split_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_df_agg(df):\n",
    "    aggs = {\n",
    "        'Store_Ratio' : ['min', 'max', 'nunique', 'size', ],\n",
    "        'Basket_Ratio' : ['min', 'max', 'nunique', 'size'],\n",
    "        'Store_Score' : ['min', 'max', 'nunique', 'size'],\n",
    "        'Store_Presence': ['min', 'max', 'nunique', 'size'],\n",
    "        'Category_2' : ['size'],\n",
    "        'Score_1' : ['min', 'max', 'nunique', 'size'],\n",
    "        'Score_2' : ['min', 'max', 'nunique', 'size'],\n",
    "        'Score_3' : ['min', 'max', 'nunique', 'size'],\n",
    "        'Score_4' : ['min', 'max', 'nunique', 'size'],\n",
    "        'time' : ['min', 'max', 'size']\n",
    "    }\n",
    "    df_agg = df.groupby('Category_1').agg(aggs)\n",
    "    df_agg.columns = ['_'.join(c).strip('_') for c in df_agg.columns]\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_agg = get_df_agg(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.merge(left=df, right=df_agg, on = 'Category_1', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test, features = split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, features = join_df(train_proc, test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mean_Score_1_Per_Popularity'] = df.groupby('Category_1')['Score_1'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc, test_proc, features = split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Fold 1*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.446704\n",
      "[200]\tvalid_0's multi_logloss: 0.430416\n",
      "[300]\tvalid_0's multi_logloss: 0.420415\n",
      "[400]\tvalid_0's multi_logloss: 0.413612\n",
      "[500]\tvalid_0's multi_logloss: 0.409002\n",
      "[600]\tvalid_0's multi_logloss: 0.406965\n",
      "[700]\tvalid_0's multi_logloss: 0.405435\n",
      "[800]\tvalid_0's multi_logloss: 0.405781\n",
      "Early stopping, best iteration is:\n",
      "[784]\tvalid_0's multi_logloss: 0.405349\n",
      "Log Loss is 0.4053485372189556\n",
      "\n",
      "*********************Fold 2*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.433872\n",
      "[200]\tvalid_0's multi_logloss: 0.416475\n",
      "[300]\tvalid_0's multi_logloss: 0.406324\n",
      "[400]\tvalid_0's multi_logloss: 0.40037\n",
      "[500]\tvalid_0's multi_logloss: 0.397801\n",
      "[600]\tvalid_0's multi_logloss: 0.396053\n",
      "[700]\tvalid_0's multi_logloss: 0.395987\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's multi_logloss: 0.395818\n",
      "Log Loss is 0.3958177716324695\n",
      "\n",
      "*********************Fold 3*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.443737\n",
      "[200]\tvalid_0's multi_logloss: 0.429181\n",
      "[300]\tvalid_0's multi_logloss: 0.422243\n",
      "[400]\tvalid_0's multi_logloss: 0.420556\n",
      "Early stopping, best iteration is:\n",
      "[416]\tvalid_0's multi_logloss: 0.420415\n",
      "Log Loss is 0.420414509710259\n",
      "\n",
      "*********************Fold 4*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.442432\n",
      "[200]\tvalid_0's multi_logloss: 0.426654\n",
      "[300]\tvalid_0's multi_logloss: 0.418953\n",
      "[400]\tvalid_0's multi_logloss: 0.413485\n",
      "[500]\tvalid_0's multi_logloss: 0.410705\n",
      "[600]\tvalid_0's multi_logloss: 0.409191\n",
      "Early stopping, best iteration is:\n",
      "[629]\tvalid_0's multi_logloss: 0.409043\n",
      "Log Loss is 0.40904310668007965\n",
      "\n",
      "*********************Fold 5*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.437362\n",
      "[200]\tvalid_0's multi_logloss: 0.41982\n",
      "[300]\tvalid_0's multi_logloss: 0.409614\n",
      "[400]\tvalid_0's multi_logloss: 0.403282\n",
      "[500]\tvalid_0's multi_logloss: 0.399752\n",
      "[600]\tvalid_0's multi_logloss: 0.397988\n",
      "[700]\tvalid_0's multi_logloss: 0.397156\n",
      "Early stopping, best iteration is:\n",
      "[746]\tvalid_0's multi_logloss: 0.397099\n",
      "Log Loss is 0.3970992673726013\n",
      "\n",
      "Log Loss on All Train Set is 0.40554491021130634\n"
     ]
    }
   ],
   "source": [
    "params = {'verbose': 100, 'early_stopping_rounds': 50}\n",
    "oofs, preds, preds_acc = run_gradient_boosting(clf, train_proc, test_proc, features, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, features = join_df(train_proc, test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Store_wise_Basket'] = df.groupby('Category_1')['Basket_Ratio'].transform('mean')\n",
    "df['Store_wise_Basket'].fillna(999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc, test_proc, features = split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5320    55\n",
       "0.7030    54\n",
       "0.6090    52\n",
       "0.6400    51\n",
       "0.7050    50\n",
       "          ..\n",
       "0.0598     1\n",
       "0.9540     1\n",
       "0.0954     1\n",
       "0.0570     1\n",
       "0.0733     1\n",
       "Name: Store_Ratio, Length: 1053, dtype: int64"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Store_Ratio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Ratio</th>\n",
       "      <th>Basket_Ratio</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Store_Score</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Store_Presence</th>\n",
       "      <th>Score_1</th>\n",
       "      <th>Score_2</th>\n",
       "      <th>Score_3</th>\n",
       "      <th>Score_4</th>\n",
       "      <th>time</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>3</td>\n",
       "      <td>-11.474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.384</td>\n",
       "      <td>84.962</td>\n",
       "      <td>143131</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>11</td>\n",
       "      <td>-13.355</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.281</td>\n",
       "      <td>139.868</td>\n",
       "      <td>120000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>11</td>\n",
       "      <td>-3.201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>0.700</td>\n",
       "      <td>130.000</td>\n",
       "      <td>241667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.413</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.737</td>\n",
       "      <td>107.088</td>\n",
       "      <td>267787</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>0.887</td>\n",
       "      <td>144.077</td>\n",
       "      <td>203760</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.043</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4340</td>\n",
       "      <td>0.865</td>\n",
       "      <td>129.961</td>\n",
       "      <td>286616</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>0.434</td>\n",
       "      <td>104.996</td>\n",
       "      <td>209733</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.880</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0827</td>\n",
       "      <td>0.463</td>\n",
       "      <td>115.478</td>\n",
       "      <td>196853</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.4190</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.198</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.554</td>\n",
       "      <td>128.909</td>\n",
       "      <td>182964</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.657</td>\n",
       "      <td>136.282</td>\n",
       "      <td>212000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>5</td>\n",
       "      <td>-12.738</td>\n",
       "      <td>1</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.310</td>\n",
       "      <td>109.981</td>\n",
       "      <td>260187</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>4</td>\n",
       "      <td>-13.843</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.145</td>\n",
       "      <td>139.815</td>\n",
       "      <td>247040</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8140</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.466</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.259</td>\n",
       "      <td>110.000</td>\n",
       "      <td>223279</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.740</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.3230</td>\n",
       "      <td>0.784</td>\n",
       "      <td>103.890</td>\n",
       "      <td>319640</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7690</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.583</td>\n",
       "      <td>107.949</td>\n",
       "      <td>213520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.535</td>\n",
       "      <td>72.916</td>\n",
       "      <td>192933</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7690</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.526</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.583</td>\n",
       "      <td>107.949</td>\n",
       "      <td>213520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8060</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.830</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.763</td>\n",
       "      <td>117.945</td>\n",
       "      <td>268693</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.657</td>\n",
       "      <td>136.282</td>\n",
       "      <td>212000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6232</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>6</td>\n",
       "      <td>-9.592</td>\n",
       "      <td>1</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>0.376</td>\n",
       "      <td>119.789</td>\n",
       "      <td>242520</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6485</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0</td>\n",
       "      <td>-15.541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.319</td>\n",
       "      <td>118.120</td>\n",
       "      <td>199037</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7440</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.871</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.726</td>\n",
       "      <td>124.002</td>\n",
       "      <td>180968</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8140</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.466</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.259</td>\n",
       "      <td>110.000</td>\n",
       "      <td>223279</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>3</td>\n",
       "      <td>-11.474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.384</td>\n",
       "      <td>84.962</td>\n",
       "      <td>143131</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7090</td>\n",
       "      <td>11</td>\n",
       "      <td>-12.649</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.853</td>\n",
       "      <td>110.522</td>\n",
       "      <td>339933</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>4</td>\n",
       "      <td>-10.156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.140</td>\n",
       "      <td>102.000</td>\n",
       "      <td>162799</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9867</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.276</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>0.235</td>\n",
       "      <td>111.057</td>\n",
       "      <td>200200</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>4</td>\n",
       "      <td>-10.156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.140</td>\n",
       "      <td>102.000</td>\n",
       "      <td>162799</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10361</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7070</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.452</td>\n",
       "      <td>117.974</td>\n",
       "      <td>207734</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10386</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.897</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>0.476</td>\n",
       "      <td>115.474</td>\n",
       "      <td>196853</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10446</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>9</td>\n",
       "      <td>-11.145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.230</td>\n",
       "      <td>80.019</td>\n",
       "      <td>144294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.7020</td>\n",
       "      <td>0.657</td>\n",
       "      <td>136.282</td>\n",
       "      <td>212000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10477</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>3</td>\n",
       "      <td>-11.474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.384</td>\n",
       "      <td>84.962</td>\n",
       "      <td>143131</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.844</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.694</td>\n",
       "      <td>161.394</td>\n",
       "      <td>205520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10744</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.579</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.819</td>\n",
       "      <td>99.986</td>\n",
       "      <td>182133</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.080</td>\n",
       "      <td>0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.457</td>\n",
       "      <td>94.996</td>\n",
       "      <td>210915</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.864</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.818</td>\n",
       "      <td>140.016</td>\n",
       "      <td>228089</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>11</td>\n",
       "      <td>-13.355</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.281</td>\n",
       "      <td>139.868</td>\n",
       "      <td>120000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7070</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.452</td>\n",
       "      <td>117.974</td>\n",
       "      <td>207734</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13113</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>5</td>\n",
       "      <td>-9.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.847</td>\n",
       "      <td>121.297</td>\n",
       "      <td>1377720</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13956</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7010</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.444</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>0.589</td>\n",
       "      <td>124.967</td>\n",
       "      <td>204373</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14073</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>11</td>\n",
       "      <td>-13.355</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.281</td>\n",
       "      <td>139.868</td>\n",
       "      <td>120000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14392</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>3</td>\n",
       "      <td>-11.474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.384</td>\n",
       "      <td>84.962</td>\n",
       "      <td>143131</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14933</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.696</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.480</td>\n",
       "      <td>125.068</td>\n",
       "      <td>166700</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.047</td>\n",
       "      <td>1</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>0.876</td>\n",
       "      <td>98.001</td>\n",
       "      <td>234640</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15405</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>2</td>\n",
       "      <td>-16.179</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.214</td>\n",
       "      <td>111.410</td>\n",
       "      <td>142000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16427</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.9090</td>\n",
       "      <td>9</td>\n",
       "      <td>-2.696</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.672</td>\n",
       "      <td>100.035</td>\n",
       "      <td>194040</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17759</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.626</td>\n",
       "      <td>1</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.325</td>\n",
       "      <td>95.073</td>\n",
       "      <td>213789</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17828</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.4120</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.203</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.307</td>\n",
       "      <td>84.904</td>\n",
       "      <td>222313</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17861</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>11</td>\n",
       "      <td>-13.326</td>\n",
       "      <td>0</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.765</td>\n",
       "      <td>94.358</td>\n",
       "      <td>308010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Store_Ratio  Basket_Ratio  Category_1  Store_Score  Category_2  \\\n",
       "9            0.705        0.2210           3      -11.474           0   \n",
       "79           0.705        0.3280          11      -13.355           0   \n",
       "312          0.705        0.8820          11       -3.201           0   \n",
       "574          0.705        0.7280           0      -11.413           0   \n",
       "583          0.705        0.6400          11       -6.928           0   \n",
       "639          0.705        0.7050           0       -7.043           1   \n",
       "704          0.705        0.7140          11       -6.390           0   \n",
       "1577         0.705        0.6160           1       -4.880           1   \n",
       "2279         0.705        0.4190           7       -9.198           1   \n",
       "2579         0.705        0.8980           5       -6.371           0   \n",
       "2903         0.705        0.3810           5      -12.738           1   \n",
       "2920         0.705        0.1880           4      -13.843           0   \n",
       "3077         0.705        0.8140           4       -5.466           0   \n",
       "3475         0.705        0.9140           7       -5.740           1   \n",
       "3534         0.705        0.7690           9       -5.526           0   \n",
       "4629         0.705        0.3470           9       -6.516           1   \n",
       "4712         0.705        0.7690           9       -5.526           0   \n",
       "4793         0.705        0.8060           0       -4.830           1   \n",
       "4915         0.705        0.8980           5       -6.371           0   \n",
       "6232         0.705        0.3540           6       -9.592           1   \n",
       "6485         0.705        0.2450           0      -15.541           0   \n",
       "7759         0.705        0.7440           8       -5.871           1   \n",
       "7933         0.705        0.8140           4       -5.466           0   \n",
       "8158         0.705        0.2210           3      -11.474           0   \n",
       "8227         0.705        0.7090          11      -12.649           0   \n",
       "8787         0.705        0.4700           4      -10.156           0   \n",
       "9867         0.705        0.6440           2       -9.276           1   \n",
       "10114        0.705        0.4700           4      -10.156           0   \n",
       "10361        0.705        0.7070           0       -5.174           1   \n",
       "10386        0.705        0.6090           1       -4.897           1   \n",
       "10446        0.705        0.2520           9      -11.145           1   \n",
       "10451        0.705        0.8980           5       -6.371           0   \n",
       "10477        0.705        0.2210           3      -11.474           0   \n",
       "10544        0.705        0.8110           8       -4.844           1   \n",
       "10744        0.705        0.7610           0       -4.579           1   \n",
       "10789        0.705        0.7340           2       -5.080           0   \n",
       "11053        0.705        0.7130           2       -8.864           0   \n",
       "11803        0.705        0.3280          11      -13.355           0   \n",
       "12926        0.705        0.7070           0       -5.174           1   \n",
       "13113        0.705        0.5670           5       -9.648           1   \n",
       "13956        0.705        0.7010           7       -3.444           1   \n",
       "14073        0.705        0.3280          11      -13.355           0   \n",
       "14392        0.705        0.2210           3      -11.474           0   \n",
       "14933        0.705        0.7700           1       -6.696           1   \n",
       "15079        0.705        0.6060           7       -7.047           1   \n",
       "15405        0.705        0.0552           2      -16.179           1   \n",
       "16427        0.705        0.9090           9       -2.696           1   \n",
       "17759        0.705        0.5620           1       -5.626           1   \n",
       "17828        0.705        0.4120           1       -9.203           1   \n",
       "17861        0.705        0.5440          11      -13.326           0   \n",
       "\n",
       "       Store_Presence   Score_1  Score_2  Score_3  Score_4     time  \\\n",
       "9            0.938000  0.906000   0.1110    0.384   84.962   143131   \n",
       "79           0.118000  0.857000   0.1830    0.281  139.868   120000   \n",
       "312          0.000369  0.000001   0.0934    0.700  130.000   241667   \n",
       "574          0.014100  0.343000   0.2450    0.737  107.088   267787   \n",
       "583          0.023700  0.000000   0.2620    0.887  144.077   203760   \n",
       "639          0.501000  0.000000   0.4340    0.865  129.961   286616   \n",
       "704          0.024900  0.000000   0.2530    0.434  104.996   209733   \n",
       "1577         0.028900  0.000000   0.0827    0.463  115.478   196853   \n",
       "2279         0.592000  0.000403   0.1580    0.554  128.909   182964   \n",
       "2579         0.207000  0.001180   0.7020    0.657  136.282   212000   \n",
       "2903         0.732000  0.616000   0.1060    0.310  109.981   260187   \n",
       "2920         0.223000  0.744000   0.1040    0.145  139.815   247040   \n",
       "3077         0.001150  0.003120   0.2030    0.259  110.000   223279   \n",
       "3475         0.081600  0.273000   0.3230    0.784  103.890   319640   \n",
       "3534         0.113000  0.000000   0.1050    0.583  107.949   213520   \n",
       "4629         0.456000  0.000796   0.0515    0.535   72.916   192933   \n",
       "4712         0.113000  0.000000   0.1050    0.583  107.949   213520   \n",
       "4793         0.229000  0.000000   0.0676    0.763  117.945   268693   \n",
       "4915         0.207000  0.001180   0.7020    0.657  136.282   212000   \n",
       "6232         0.479000  0.000000   0.0734    0.376  119.789   242520   \n",
       "6485         0.869000  0.921000   0.1110    0.319  118.120   199037   \n",
       "7759         0.017000  0.309000   0.2830    0.726  124.002   180968   \n",
       "7933         0.001150  0.003120   0.2030    0.259  110.000   223279   \n",
       "8158         0.938000  0.906000   0.1110    0.384   84.962   143131   \n",
       "8227         0.119000  0.000017   0.2860    0.853  110.522   339933   \n",
       "8787         0.335000  0.863000   0.1150    0.140  102.000   162799   \n",
       "9867         0.545000  0.043400   0.0997    0.235  111.057   200200   \n",
       "10114        0.335000  0.863000   0.1150    0.140  102.000   162799   \n",
       "10361        0.079900  0.000000   0.1490    0.452  117.974   207734   \n",
       "10386        0.025700  0.000000   0.0846    0.476  115.474   196853   \n",
       "10446        0.935000  0.917000   0.1010    0.230   80.019   144294   \n",
       "10451        0.207000  0.001180   0.7020    0.657  136.282   212000   \n",
       "10477        0.938000  0.906000   0.1110    0.384   84.962   143131   \n",
       "10544        0.297000  0.000000   0.1650    0.694  161.394   205520   \n",
       "10744        0.003850  0.000000   0.1860    0.819   99.986   182133   \n",
       "10789        0.280000  0.000071   0.0930    0.457   94.996   210915   \n",
       "11053        0.245000  0.000537   0.2750    0.818  140.016   228089   \n",
       "11803        0.118000  0.857000   0.1830    0.281  139.868   120000   \n",
       "12926        0.079900  0.000000   0.1490    0.452  117.974   207734   \n",
       "13113        0.432000  0.000000   0.2840    0.847  121.297  1377720   \n",
       "13956        0.029000  0.000000   0.1790    0.589  124.967   204373   \n",
       "14073        0.118000  0.857000   0.1830    0.281  139.868   120000   \n",
       "14392        0.938000  0.906000   0.1110    0.384   84.962   143131   \n",
       "14933        0.010400  0.000000   0.0606    0.480  125.068   166700   \n",
       "15079        0.159000  0.001490   0.2420    0.876   98.001   234640   \n",
       "15405        0.978000  0.928000   0.1060    0.214  111.410   142000   \n",
       "16427        0.082800  0.000000   0.0712    0.672  100.035   194040   \n",
       "17759        0.645000  0.000000   0.0669    0.325   95.073   213789   \n",
       "17828        0.416000  0.000000   0.1050    0.307   84.904   222313   \n",
       "17861        0.749000  0.000034   0.3470    0.765   94.358   308010   \n",
       "\n",
       "       popularity  \n",
       "9               3  \n",
       "79              3  \n",
       "312             3  \n",
       "574             4  \n",
       "583             3  \n",
       "639             3  \n",
       "704             3  \n",
       "1577            3  \n",
       "2279            3  \n",
       "2579            3  \n",
       "2903            3  \n",
       "2920            3  \n",
       "3077            3  \n",
       "3475            3  \n",
       "3534            3  \n",
       "4629            3  \n",
       "4712            3  \n",
       "4793            3  \n",
       "4915            3  \n",
       "6232            2  \n",
       "6485            3  \n",
       "7759            3  \n",
       "7933            3  \n",
       "8158            3  \n",
       "8227            3  \n",
       "8787            3  \n",
       "9867            3  \n",
       "10114           3  \n",
       "10361           3  \n",
       "10386           3  \n",
       "10446           3  \n",
       "10451           3  \n",
       "10477           3  \n",
       "10544           3  \n",
       "10744           3  \n",
       "10789           3  \n",
       "11053           3  \n",
       "11803           3  \n",
       "12926           3  \n",
       "13113           3  \n",
       "13956           3  \n",
       "14073           3  \n",
       "14392           3  \n",
       "14933           3  \n",
       "15079           3  \n",
       "15405           2  \n",
       "16427           3  \n",
       "17759           3  \n",
       "17828           3  \n",
       "17861           3  "
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[ train['Store_Ratio'] == 0.7050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Fold 1*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.446245\n",
      "[200]\tvalid_0's multi_logloss: 0.42968\n",
      "[300]\tvalid_0's multi_logloss: 0.419931\n",
      "[400]\tvalid_0's multi_logloss: 0.413085\n",
      "[500]\tvalid_0's multi_logloss: 0.409202\n",
      "[600]\tvalid_0's multi_logloss: 0.40746\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's multi_logloss: 0.407306\n",
      "Log Loss is 0.40730595559899724\n",
      "\n",
      "*********************Fold 2*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.435025\n",
      "[200]\tvalid_0's multi_logloss: 0.418088\n",
      "[300]\tvalid_0's multi_logloss: 0.40809\n",
      "[400]\tvalid_0's multi_logloss: 0.401446\n",
      "[500]\tvalid_0's multi_logloss: 0.397606\n",
      "[600]\tvalid_0's multi_logloss: 0.396287\n",
      "Early stopping, best iteration is:\n",
      "[613]\tvalid_0's multi_logloss: 0.396057\n",
      "Log Loss is 0.3960568881281105\n",
      "\n",
      "*********************Fold 3*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.442814\n",
      "[200]\tvalid_0's multi_logloss: 0.428187\n",
      "[300]\tvalid_0's multi_logloss: 0.422547\n",
      "[400]\tvalid_0's multi_logloss: 0.419842\n",
      "[500]\tvalid_0's multi_logloss: 0.419549\n",
      "Early stopping, best iteration is:\n",
      "[464]\tvalid_0's multi_logloss: 0.419103\n",
      "Log Loss is 0.41910345440965063\n",
      "\n",
      "*********************Fold 4*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.441722\n",
      "[200]\tvalid_0's multi_logloss: 0.426421\n",
      "[300]\tvalid_0's multi_logloss: 0.417278\n",
      "[400]\tvalid_0's multi_logloss: 0.412693\n",
      "[500]\tvalid_0's multi_logloss: 0.410269\n",
      "[600]\tvalid_0's multi_logloss: 0.409703\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's multi_logloss: 0.409404\n",
      "Log Loss is 0.4094038300546094\n",
      "\n",
      "*********************Fold 5*********************\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.436331\n",
      "[200]\tvalid_0's multi_logloss: 0.418509\n",
      "[300]\tvalid_0's multi_logloss: 0.408853\n",
      "[400]\tvalid_0's multi_logloss: 0.403331\n",
      "[500]\tvalid_0's multi_logloss: 0.399913\n",
      "[600]\tvalid_0's multi_logloss: 0.398622\n",
      "[700]\tvalid_0's multi_logloss: 0.39872\n",
      "Early stopping, best iteration is:\n",
      "[661]\tvalid_0's multi_logloss: 0.39836\n",
      "Log Loss is 0.39836017317068234\n",
      "\n",
      "Log Loss on All Train Set is 0.4060462979765686\n"
     ]
    }
   ],
   "source": [
    "params = {'verbose': 100, 'early_stopping_rounds': 50}\n",
    "oofs, preds, preds_acc = run_gradient_boosting(clf, train_proc, test_proc, features, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29319487, 0.27543043, 0.58655882, ..., 0.00841   , 0.0804    ,\n",
       "       0.144     ])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Store_wise_Basket'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Ratio</th>\n",
       "      <th>Basket_Ratio</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Store_Score</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Store_Presence</th>\n",
       "      <th>Score_1</th>\n",
       "      <th>Score_2</th>\n",
       "      <th>Score_3</th>\n",
       "      <th>Score_4</th>\n",
       "      <th>time</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0.0601</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>53.689</td>\n",
       "      <td>161309</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>9</td>\n",
       "      <td>-18.874</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>0.05880</td>\n",
       "      <td>87.062</td>\n",
       "      <td>146624</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>10</td>\n",
       "      <td>-22.153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.01210</td>\n",
       "      <td>54.263</td>\n",
       "      <td>181783</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>9</td>\n",
       "      <td>-27.077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>62.749</td>\n",
       "      <td>209455</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0</td>\n",
       "      <td>-26.208</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.03960</td>\n",
       "      <td>71.476</td>\n",
       "      <td>413999</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store_Ratio  Basket_Ratio  Category_1  Store_Score  Category_2  \\\n",
       "748        0.0601        1.0000           2      -14.825           0   \n",
       "823        0.0937        0.1570           9      -18.874           1   \n",
       "911        0.0563        0.7990          10      -22.153           0   \n",
       "1005       0.0936        0.0697           9      -27.077           0   \n",
       "1151       0.0999        0.0604           0      -26.208           1   \n",
       "\n",
       "      Store_Presence  Score_1  Score_2  Score_3  Score_4    time  popularity  \n",
       "748           0.1210    0.976   0.9310  0.00001   53.689  161309         NaN  \n",
       "823           0.8170    0.890   0.0940  0.05880   87.062  146624         NaN  \n",
       "911           0.0236    0.928   0.5370  0.01210   54.263  181783         NaN  \n",
       "1005          0.9720    0.915   0.0984  0.11100   62.749  209455         NaN  \n",
       "1151          0.9470    0.955   0.1970  0.03960   71.476  413999         NaN  "
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test[target].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Ratio</th>\n",
       "      <th>Basket_Ratio</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Store_Score</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Store_Presence</th>\n",
       "      <th>Score_1</th>\n",
       "      <th>Score_2</th>\n",
       "      <th>Score_3</th>\n",
       "      <th>Score_4</th>\n",
       "      <th>time</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.164</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.718</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.99100</td>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>127.602</td>\n",
       "      <td>236436</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586</td>\n",
       "      <td>0.636</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>0.32200</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.35500</td>\n",
       "      <td>136.337</td>\n",
       "      <td>253631</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.591</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.00931</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>0.68800</td>\n",
       "      <td>154.902</td>\n",
       "      <td>215669</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.423</td>\n",
       "      <td>0.748</td>\n",
       "      <td>5</td>\n",
       "      <td>-9.832</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.42800</td>\n",
       "      <td>93.977</td>\n",
       "      <td>325200</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.756</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.791</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08430</td>\n",
       "      <td>0.00765</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>0.96300</td>\n",
       "      <td>131.715</td>\n",
       "      <td>288293</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store_Ratio  Basket_Ratio  Category_1  Store_Score  Category_2  \\\n",
       "0        0.164         0.994           1      -23.718           0   \n",
       "1        0.586         0.636           4       -7.710           1   \n",
       "2        0.457         0.743           0       -7.591           1   \n",
       "3        0.423         0.748           5       -9.832           1   \n",
       "4        0.802         0.756           2      -10.791           0   \n",
       "\n",
       "   Store_Presence  Score_1  Score_2  Score_3  Score_4    time  popularity  \n",
       "0         0.12400  0.99100   0.7890  0.00001  127.602  236436         2.0  \n",
       "1         0.00208  0.32200   0.1070  0.35500  136.337  253631         3.0  \n",
       "2         0.40400  0.00931   0.2010  0.68800  154.902  215669         3.0  \n",
       "3         0.03000  0.22100   0.1690  0.42800   93.977  325200         3.0  \n",
       "4         0.08430  0.00765   0.0521  0.96300  131.715  288293         3.0  "
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
